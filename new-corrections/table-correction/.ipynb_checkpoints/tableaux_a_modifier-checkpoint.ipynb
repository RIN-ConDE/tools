{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production de tableaux pour correction des textes tokenisés sur Excel/Calc\n",
    "\n",
    "Ce script permet de transformer des textes tokénisés en format XML-TEI avec des `//w[@lemma and @pos and @n]`.\n",
    "Il nécessite des XML encodés selon le schéma ConDÉ, en version `base`.\n",
    "Il comporte deux fonctions :\n",
    "* get_w_txt(), qui construit les chaînes de caractères de chaque token,\n",
    "* export_tokens_to_csv(), qui parse le XML, extrait les informations et ensuite les écrit dans un CSV.\n",
    "Les dernières cellules donnent des emplacements pour lancer export_tokens_to_csv().\n",
    "\n",
    "-----------------\n",
    "\n",
    "# Production of tables to correct tokenized texts on Excel/Calc\n",
    "\n",
    "This script transforms XML-TEI texts tokenized as `//w[@lemma and @pos and @n]`.\n",
    "It needs XMLs encoded according to the ConDÉ `base` version schema.\n",
    "It is written in two functions:\n",
    "* get_w_txt(), which constructs strings out of each token,\n",
    "* export_tokens_to_csv(), which parses the XML, extracts information and then writes it into a CSV file.\n",
    "The last cells provide a place to launch export_tokens_to_csv()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and declare TEI namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Declaring base namespace (TEI),\n",
    "# without a prefix (since it's the base namespace).\n",
    "ET.register_namespace('', \"http://www.tei-c.org/ns/1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting word strings from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_txt(word):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function constructing two strings from one word token (tei:w),\n",
    "    first string being the 'diplomatic' version of the word,\n",
    "    second string being the modernized/corrected version.\n",
    "    If the word does not change, both strings will be identical.\n",
    "    Function is based on the fact that each token has at least one child:\n",
    "    it loops on all children of the word token.\n",
    "    \n",
    "    Function returns texte (list). texte[1] will always be the modernized/\n",
    "    constructed version.\n",
    "    If both constructed strings are identical, texte[0] will be an empty\n",
    "    string. Otherwise it will contain the 'diplomatic' version.\n",
    "    \n",
    "    :param word: A parsed XML object corresponding to the following\n",
    "    path: //tei:w.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The future 'diplomatic' version.\n",
    "    amorig = []\n",
    "    # The future 'modernized' version.\n",
    "    expmod = []\n",
    "    \n",
    "    # Storing TEI elements which may be treated the same.\n",
    "    checklist = [\n",
    "        '{http://www.tei-c.org/ns/1.0}height',\n",
    "        '{http://www.tei-c.org/ns/1.0}supplied',\n",
    "        '{http://www.tei-c.org/ns/1.0}c',\n",
    "        '{http://www.tei-c.org/ns/1.0}hi'\n",
    "    ]\n",
    "    \n",
    "    # If there is text before first child, add it to both versions.\n",
    "    if word.text:\n",
    "        \n",
    "        amorig.append(word.text)\n",
    "        expmod.append(word.text)\n",
    "    \n",
    "    # Looping on children of word token. According to the nature of\n",
    "    # each child, treatment will be different.\n",
    "    for item in word:\n",
    "        \n",
    "        # For all tei:height, tei:supplied, tei:c, tei:hi,\n",
    "        # there is no modernization : add the text inside and\n",
    "        # right after closing tag to both versions.\n",
    "        if item.tag in checklist:\n",
    "            amorig.append(item.text)\n",
    "            expmod.append(item.text)\n",
    "            if item.tail:\n",
    "                amorig.append(item.tail)\n",
    "                expmod.append(item.tail)\n",
    "        \n",
    "        # If child is a line beginning, there is no text inside:\n",
    "        # if there is text directly afterwards, add it to both versions.\n",
    "        elif item.tag == '{http://www.tei-c.org/ns/1.0}lb':\n",
    "            if item.tail:\n",
    "                amorig.append(item.tail)\n",
    "                expmod.append(item.tail)\n",
    "        \n",
    "        # If child is a tei:choice element, then text will be different for\n",
    "        # both versions. We parse its own children. Contents of tei:am and\n",
    "        # tei:orig will go to 'diplomatic' version, while contents of\n",
    "        # tei:expan and tei:reg will go to 'modernized' version. If there is\n",
    "        # text directly after tei:choice element, add it to both.\n",
    "        elif item.tag == '{http://www.tei-c.org/ns/1.0}choice':\n",
    "            \n",
    "            for subitem in item.findall('./*'):\n",
    "                if subitem.tag == '{http://www.tei-c.org/ns/1.0}am' or subitem.tag == '{http://www.tei-c.org/ns/1.0}orig':\n",
    "                    amorig.append(subitem.text)\n",
    "                            \n",
    "                elif subitem.tag == '{http://www.tei-c.org/ns/1.0}expan' or subitem.tag == '{http://www.tei-c.org/ns/1.0}reg':\n",
    "                    try:\n",
    "                        expmod.append(subitem.text)\n",
    "                    except:\n",
    "                        expmod.append(\"[X]\")\n",
    "                \n",
    "            if item.tail:\n",
    "                amorig.append(item.tail)\n",
    "                expmod.append(item.tail)\n",
    "                \n",
    "        \n",
    "        # If child is tei:add, then any of the previous children may be inside.\n",
    "        # So we do the same there.\n",
    "        elif item.tag == '{http://www.tei-c.org/ns/1.0}add':\n",
    "            for subitem in item:\n",
    "    \n",
    "                if subitem.tag in checklist:\n",
    "                    amorig.append(subitem.text)\n",
    "                    expmod.append(subitem.text)\n",
    "                    if item.tail:\n",
    "                        amorig.append(subitem.tail)\n",
    "                        expmod.append(subitem.tail)\n",
    "\n",
    "                elif subitem.tag == '{http://www.tei-c.org/ns/1.0}lb':\n",
    "                    if subitem.tail:\n",
    "                        amorig.append(subitem.tail)\n",
    "                        expmod.append(subitem.tail)\n",
    "\n",
    "                elif subitem.tag == '{http://www.tei-c.org/ns/1.0}choice':\n",
    "            \n",
    "                    for subsub in subitem.findall('./*'):\n",
    "                        if subsub.tag == '{http://www.tei-c.org/ns/1.0}am' or subsub.tag == '{http://www.tei-c.org/ns/1.0}orig':\n",
    "                            amorig.append(subsub.text)\n",
    "                            \n",
    "                        elif subsub.tag == '{http://www.tei-c.org/ns/1.0}expan' or subsub.tag == '{http://www.tei-c.org/ns/1.0}reg':\n",
    "                            expmod.append(subsub.text)\n",
    "\n",
    "                            if subitem.tail:\n",
    "                                amorig.append(subitem.tail)\n",
    "                                expmod.append(subitem.tail)\n",
    "    \n",
    "    # Construction of return list. If 'diplomatic' and 'modernized' versions\n",
    "    # are different, then make a list with previous and latter. Otherwise,\n",
    "    # replace previous with empty string.\n",
    "    if ''.join(amorig) != ''.join(expmod):\n",
    "        texte = [''.join(amorig), ''.join(expmod)]\n",
    "    else:\n",
    "        texte = ['', ''.join(expmod)]\n",
    "    \n",
    "    return texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tokens_to_csv(chemin_entree, chemin_sortie):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function parsing a tokenized TEI-XML file and returning a CSV\n",
    "    table allowing manual correction outside of the XML file.\n",
    "    Function uses the above get_w_txt() function to extract text\n",
    "    from tokens containing children.\n",
    "    \n",
    "    :param chemin_entree: The local path to the TEI-XML file\n",
    "        one needs to convert to a table.\n",
    "    :param chemin_sortie: The local path to where one wants the\n",
    "        resulting CSV table written.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Counter to number table lines independantly from token numbers. \n",
    "    l_count = 0\n",
    "    \n",
    "    # List used to store together //w/@n and //lb/@facs in document order,\n",
    "    # so as to render the order of elements in the table.\n",
    "    lbinitlist = []\n",
    "    \n",
    "    # Dictionnary used to store final information. Keys are w numbers //w/@n,\n",
    "    # contents are all informations needed to make CSV file, including\n",
    "    # //following::sibling:*[1][self::lb] or //child::lb when such is the case.\n",
    "    winitdict = {}\n",
    "    \n",
    "    # Columns of final CSV.\n",
    "    colonnes = [\n",
    "        'n° de ligne',\n",
    "        'nature',\n",
    "        'n°/id',\n",
    "        'forme \"diplo\"',\n",
    "        'forme modernisée',\n",
    "        'forme corrigée',\n",
    "        'lemme',\n",
    "        'pos',\n",
    "        'à scinder',\n",
    "        'à fusionner avec w n°',\n",
    "        'à corriger',\n",
    "        'corrigé'\n",
    "    ]\n",
    "    \n",
    "    # Open and parse TEI XML.\n",
    "    with open(chemin_entree) as infile:\n",
    "    \n",
    "        tree = ET.parse(infile)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Loop on all elements containing //w children.\n",
    "        # Add @n to lbinitlist if children are //w,\n",
    "        # add @facs it children are //lb.\n",
    "        # Order of elements in documents is preserved here.\n",
    "        current = time.time_ns()\n",
    "        for parent in root.findall('.//*[{http://www.tei-c.org/ns/1.0}w]'):\n",
    "            for child in parent.findall('./*'):\n",
    "                if child.tag == '{http://www.tei-c.org/ns/1.0}w':\n",
    "                    lbinitlist.append(child.get('n'))\n",
    "                elif child.tag == '{http://www.tei-c.org/ns/1.0}lb':\n",
    "                    lbinitlist.append(child.get('facs'))\n",
    "        print(f\"Liste des LB: {(time.time_ns() - current)/1000000000} s.\")\n",
    "        current = time.time_ns()\n",
    "        # Loop on //w elements. Register @n, @lemma and @pos and start\n",
    "        # the dictionary entry in winitdict.\n",
    "        for word in root.findall('.//{http://www.tei-c.org/ns/1.0}w'):\n",
    "            texte = ''\n",
    "            numero = str(word.get('n'))\n",
    "            lemmes = str(word.get('lemma'))\n",
    "            pos = str(word.get('pos'))\n",
    "            \n",
    "            winitdict[numero] = {'lemma':lemmes, 'pos':pos}\n",
    "            \n",
    "            # Then check if word has an //lb child (which would\n",
    "            # not appear in lbinitlist) and, if so, add it to the dictionary\n",
    "            # entry.\n",
    "            if word.find('{http://www.tei-c.org/ns/1.0}lb'):\n",
    "                winitdict[numero]['lb'] = word.find('{http://www.tei-c.org/ns/1.0}lb').get('facs')\n",
    "            \n",
    "            # Otherwise, check for the @n in lbinitlist, see if it is\n",
    "            # followed by a //lb/@facs. If so, add it to the dictionary\n",
    "            # entry. Otherwise, write 'None'.\n",
    "            else:\n",
    "                wIndex = lbinitlist.index(numero)\n",
    "                try:\n",
    "                    nextIndex = lbinitlist[wIndex+1]\n",
    "                except:\n",
    "                    nextIndex = ''\n",
    "                if \"_\"in nextIndex:\n",
    "                    winitdict[numero]['lb'] = nextIndex\n",
    "                else:\n",
    "                    winitdict[numero]['lb'] = 'None'\n",
    "            \n",
    "            # If word token has no children, we can get the\n",
    "            # text directly. Otherwise, invoque get_w_txt() function\n",
    "            # to compose it.\n",
    "            if word.find('./*') == None :\n",
    "                winitdict[numero]['original'] = ''\n",
    "                winitdict[numero]['modernisé'] = word.text\n",
    "\n",
    "            else:\n",
    "                winitdict[numero]['original'] = get_w_txt(word)[0]\n",
    "                winitdict[numero]['modernisé'] = get_w_txt(word)[1]\n",
    "\n",
    "        print(f\"Dico des W: {(time.time_ns() - current)/1000000000} s.\")\n",
    "    current = time.time_ns()\n",
    "    # Open a CSV file in the output path, parse and write column headers.\n",
    "    with open(chemin_sortie, 'w') as csv_file:\n",
    "        csv_contenu = csv.DictWriter(csv_file, fieldnames = colonnes)\n",
    "        csv_contenu.writeheader()\n",
    "\n",
    "        # Then loop on word tokens in winitdict.\n",
    "        for word in winitdict.keys():\n",
    "            \n",
    "            dicolocal = winitdict[word]\n",
    "\n",
    "            l_count += 1\n",
    "            \n",
    "            # Check if token is ambiguous for sure and make the corresponding\n",
    "            # variable.\n",
    "            if '|' in dicolocal['pos'] or dicolocal['pos'] == 'Inconnu':\n",
    "                a_corriger = 'X'\n",
    "            else:\n",
    "                a_corriger = ''\n",
    "\n",
    "            \n",
    "            # If current token dictionary contains a //lb/@facs, then\n",
    "            # we add two lines: one for the line beginning, the other\n",
    "            # for the actual word token. In-between, we add 1 to the\n",
    "            # general counter.\n",
    "            # Otherwise, we only add one line to the CSV table.\n",
    "            if dicolocal['lb'] != 'None':\n",
    "                csv_contenu.writerow({\n",
    "                    'n° de ligne' : str(l_count),\n",
    "                    'nature' : 'saut de ligne',\n",
    "                    'n°/id' : dicolocal['lb'],\n",
    "                    'forme \"diplo\"' :'',\n",
    "                    'forme modernisée':'',\n",
    "                    'forme corrigée':'',\n",
    "                    'lemme' :'',\n",
    "                    'pos' :'',\n",
    "                    'à scinder' :'',\n",
    "                    'à fusionner avec w n°' :'',\n",
    "                    'à corriger' :'',\n",
    "                    'corrigé' :''\n",
    "                })\n",
    "\n",
    "                l_count += 1\n",
    "\n",
    "                csv_contenu.writerow({\n",
    "                    'n° de ligne' : str(l_count),\n",
    "                    'nature' : 'w',\n",
    "                    'n°/id' : word,\n",
    "                    'forme \"diplo\"' : dicolocal['original'],\n",
    "                    'forme modernisée' : dicolocal['modernisé'],\n",
    "                    'forme corrigée':'',\n",
    "                    'lemme' : dicolocal['lemma'],\n",
    "                    'pos' : dicolocal['pos'],\n",
    "                    'à scinder' :'',\n",
    "                    'à fusionner avec w n°' :'',\n",
    "                    'à corriger' : a_corriger,\n",
    "                    'corrigé' :''\n",
    "                })\n",
    "\n",
    "            else:\n",
    "\n",
    "                csv_contenu.writerow({\n",
    "                    'n° de ligne' : str(l_count),\n",
    "                    'nature' : 'w',\n",
    "                    'n°/id' : word,\n",
    "                    'forme \"diplo\"' : dicolocal['original'],\n",
    "                    'forme modernisée' : dicolocal['modernisé'],\n",
    "                    'forme corrigée':'',\n",
    "                    'lemme' : dicolocal['lemma'],\n",
    "                    'pos' : dicolocal['pos'],\n",
    "                    'à scinder' :'',\n",
    "                    'à fusionner avec w n°' :'',\n",
    "                    'à corriger' : a_corriger,\n",
    "                    'corrigé' :''\n",
    "                })\n",
    "    print(f\"Écriture du CSV: {(time.time_ns() - current)/1000000000} s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where to launch the script for the whole collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basnage : terminé\n"
     ]
    }
   ],
   "source": [
    "# If one wishes to treat several XML documents together,\n",
    "# and they are named on the same model,\n",
    "# and they are in the same directory,\n",
    "# one can add each specific part of the filename\n",
    "# into the following list.\n",
    "temoins = ['bookname1', 'bookname2']\n",
    "\n",
    "# Then one may add the actual paths and suffixes to this loop.\n",
    "for temoin in temoins:\n",
    "    export_tokens_to_csv('/path/to/folder/' + temoin + 'suffixe.xml',\n",
    "                 temoin + '_tableau_pour_corrections.csv')\n",
    "    \n",
    "    # This allows the user to check which files are done yet.\n",
    "    print(temoin + \" : terminé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where to launch the script for one witness\n",
    "\n",
    "```\n",
    "export_tokens_to_csv(\n",
    "    '/path/to/original.xml',\n",
    "    '/path/to/table.csv'\n",
    ")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des LB: 0.0221404\n",
      "Dico des W: 29.227740066 ns\n",
      "Écriture du CSV: 0.175281253\n"
     ]
    }
   ],
   "source": [
    "export_tokens_to_csv(\n",
    "    '../../../editions/base-version/gc_base.xml',\n",
    "    'base-essai.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### À Faire\n",
    "\n",
    "* Faire le dictionnaire CSV pour les lignes directement et ensuite l'écrire.\n",
    "* Faire la vérification dans le dictionnaire des LB pendant la boucle.\n",
    "\n",
    "###### Références\n",
    "\n",
    "* https://twitter.com/MorganePica/status/1470339783729819656\n",
    "* http://python-simple.com/python-pandas/creation-dataframes.php\n",
    "* https://www.geeksforgeeks.org/python-ways-to-add-row-columns-in-numpy-array/\n",
    "* https://lxml.de/index.html\n",
    "\n",
    "###### Remarques de TC :\n",
    "> Trois quatre choses:\n",
    "> * (1) LXML est beaucoup BEAUCOUP plus rapide que l'implémentation basique.\n",
    "> * (2) Si tes données passent bien dans un \"tableur\" et que ce qui prend du temps, c'est aussi le retraitement des données -> import pandas\n",
    "> * (3) Au bout d'un moment, t'intéresser à multithreading si ça suffit pas.\n",
    "> * (4) T'assurer que tu as bien optimisé tes boucles, que tu \"recompiles\" pas des choses dans tes boucles, etc. afin de gagner en vitesse. Potentiellement, chercher où sa ralentit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
